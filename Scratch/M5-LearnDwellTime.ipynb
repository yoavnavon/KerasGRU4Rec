{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhMT2zYloMpm"
   },
   "source": [
    "## Implementación en Keras de Session-Based RNNs for Recommendation\n",
    "\n",
    "### Utilizacion de Dwell Time como feature implicita para mejor rendimiento\n",
    "\n",
    "To-Do: aprender a predecir el boosting como tarea auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3kJRW-qQ17_Q",
    "outputId": "1e336f48-43aa-4929-cefd-63e02dee3449"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import psutil\n",
    "import humanize\n",
    "import GPUtil as GPU\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "import warnings\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import cosine_proximity, categorical_crossentropy, mean_squared_error, mean_absolute_error\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers.core import Permute, Reshape, RepeatVector\n",
    "from keras.layers import Input, Dense, Dropout, GRU, CuDNNGRU, Embedding, concatenate, Lambda, multiply, merge, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfW00EfwSNQ6"
   },
   "outputs": [],
   "source": [
    "# Cargamos dataframes preprocesados\n",
    "PATH_TO_TRAIN = '../DwellTimeTheano/augmented.csv'\n",
    "PATH_TO_DEV = '../processedData/rsc15_train_valid.txt'\n",
    "PATH_TO_TEST = '../processedData/rsc15_test.txt'\n",
    "#PATH_TO_TESTSSS = '../processedData/rsc15_test.txt'\n",
    "\n",
    "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
    "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
    "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['DwellReps'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionDataset:\n",
    "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: path of the csv file\n",
    "            sep: separator for the csv\n",
    "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
    "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
    "            itemmap: mapping between item IDs and item indices\n",
    "            time_sort: whether to sort the sessions by time or not\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        self.add_item_indices(itemmap=itemmap)\n",
    "        self.df.sort_values([session_key, time_key], inplace=True)\n",
    "\n",
    "        #Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
    "        #clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
    "\n",
    "        self.click_offsets = self.get_click_offsets()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "        \n",
    "    def get_click_offsets(self):\n",
    "        \"\"\"\n",
    "        Return the offsets of the beginning clicks of each session IDs,\n",
    "        where the offset is calculated against the first click of the first session ID.\n",
    "        \"\"\"\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
    "        # group & sort the df by session_key and get the offset values\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "\n",
    "        return offsets\n",
    "    \n",
    "\n",
    "    def order_session_idx(self):\n",
    "        \"\"\" Order the session indices \"\"\"\n",
    "        if self.time_sort:\n",
    "            # starting time for each sessions, sorted by session IDs\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            # order the session indices by session starting times\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "\n",
    "        return session_idx_arr\n",
    "    \n",
    "    \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        \"\"\" \n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        \"\"\"\n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
    "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
    "                                 index=item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
    "                                   'item_idx':item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
    "        \n",
    "    \n",
    "    @property    \n",
    "    def items(self):\n",
    "        return self.itemmap.ItemId.unique()\n",
    "        \n",
    "\n",
    "class SessionDataLoader:\n",
    "    def __init__(self, dataset, batch_size=50, test=False):\n",
    "        \"\"\"\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        Args:\n",
    "             dataset (SessionDataset): the session dataset to generate the batches from\n",
    "             batch_size (int): size of the batch\n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.done_sessions_counter = 0\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
    "        Yields:\n",
    "            input (B,): Item indices that will be encoded as one-hot vectors later.\n",
    "            target (B,): a Variable that stores the target item indices\n",
    "            masks: np array indicating the positions of the sessions to be terminated\n",
    "        \"\"\"\n",
    "\n",
    "        # initializations\n",
    "        df = self.dataset.df\n",
    "        session_key='SessionId'\n",
    "        item_key='ItemId'\n",
    "        time_key='TimeStamp'\n",
    "        self.n_items = df[item_key].nunique()+1\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = [] # indicator for the sessions to be terminated\n",
    "        finished = False        \n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            # Item indices(for embedding) for clicks where the first sessions start\n",
    "            if not self.test:\n",
    "                idx_target = [df.item_idx.values[start], df.DwellReps.values[start]]\n",
    "            else:\n",
    "                idx_target = df.item_idx.values[start]\n",
    "                \n",
    "            for i in range(minlen - 1):\n",
    "                # Build inputs & targets\n",
    "                \n",
    "                # For Dwell Reps\n",
    "                # df.iloc[[idx1, idx2, ...], :][dwell_key]\n",
    "                \n",
    "                idx_input = idx_target\n",
    "                \n",
    "                if not self.test:\n",
    "                    idx_target = [df.item_idx.values[start + i + 1], df.DwellReps.values[start + i + 1]]\n",
    "                else:\n",
    "                    idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = idx_input\n",
    "                target = idx_target\n",
    "                \n",
    "                yield input, target, mask\n",
    "                \n",
    "            # click indices where a particular session meets second-to-last element\n",
    "            start = start + (minlen - 1)\n",
    "            # see if how many sessions should terminate\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            self.done_sessions_counter = len(mask)\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                # update the next starting/ending point\n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "eQslL5CpRjdQ",
    "outputId": "bd8d7ef2-638d-42a1-eaf5-6e96636a7be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items unicos training: 6360\n",
      "Items unicos dev: 6752\n",
      "Sesiones training: 12372\n",
      "Sesiones validation: 15324\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512 #como en el paper\n",
    "session_max_len = 100\n",
    "embeddingp=False\n",
    "\n",
    "n_items = len(train_data['ItemId'].unique())+1\n",
    "print(\"Items unicos training:\", n_items)\n",
    "\n",
    "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
    "print(\"Items unicos dev:\", dev_n_items)\n",
    "\n",
    "test_n_items = len(test_data['ItemId'].unique())+1\n",
    "print(\"Items unicos testing:\", test_n_items)\n",
    "\n",
    "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
    "print(\"Sesiones training:\", train_samples_qty)\n",
    "\n",
    "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
    "print(\"Sesiones validation:\", dev_samples_qty)\n",
    "\n",
    "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
    "print(\"Sesiones testing:\", test_samples_qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i_adI_ASgDi"
   },
   "outputs": [],
   "source": [
    "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
    "dev_fraction = 1#2\n",
    "\n",
    "train_offset_step=train_samples_qty//batch_size\n",
    "dev_offset_step=dev_samples_qty//batch_size\n",
    "test_offset_step=test_samples_qty//batch_size\n",
    "\n",
    "\n",
    "#aux = [0]\n",
    "#aux.extend(list(train_data['ItemId'].unique()))\n",
    "#itemids = np.array(aux)\n",
    "#itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "O5_sa72xSF50",
    "outputId": "b1a69ec9-6e86-44ba-d19e-69fe1f59cf57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (512, 1, 6360)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(512, 100), (512, 1 1938300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (512, 100)           0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (512, 6360)          642360      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (512, 1)             101         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,580,761\n",
      "Trainable params: 2,580,761\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pato/.virtualenvs/ma_mysql/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# Modelo\n",
    "\n",
    "# TODO: Force integer-only prediction. Right now it's a regression, that is, floats.\n",
    "\n",
    "# Custom loss function, forces model to jointly learn next item AND dwell time. Weights are tricky\n",
    "# Max dwell time is required, for normalization.\n",
    "def custom_loss_wrap(max_dwell_time, session_coeff=1, dwell_coeff=5):\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        #session_true, dwell_true = y_true\n",
    "        #session_pred, dwell_pred = y_pred\n",
    "        session_loss = categorical_crossentropy(y_true[0], y_pred[0])\n",
    "        dwell_loss = mean_absolute_error(y_true[1], y_pred[1])/max_dwell_time\n",
    "        \n",
    "        return session_coeff*session_loss + dwell_coeff*dwell_loss\n",
    "    return custom_loss\n",
    "    \n",
    "   \n",
    "USE_GPU=True\n",
    "    \n",
    "emb_size = 50\n",
    "hidden_units = 100\n",
    "size = emb_size\n",
    "\n",
    "# Model receives \n",
    "# Session info (standard procedure)\n",
    "# but also actual reps for augmenting, only we won't really augment\n",
    "# It should learn to predict the next item relative importance\n",
    "# given the past dwell times\n",
    "session_input = Input(batch_shape=(batch_size, 1, n_items))\n",
    "#dwell_input = Input(batch_shape=(batch_size, 1)) # Not included yet. However, it might be key for the model to learn.\n",
    "inputs = session_input # , dwell_input]\n",
    "\n",
    "if USE_GPU:\n",
    "    gru, gru_states = CuDNNGRU(hidden_units, stateful=True, return_state=True)(inputs)# drop1) #\n",
    "else:\n",
    "    gru, gru_states = GRU(hidden_units, stateful=True, return_state=True)(inputs)\n",
    "drop2 = Dropout(0.25)(gru)\n",
    "\n",
    "# Model predicts next session AND dwell time\n",
    "# The former is evaluated with categorical cross_entropy\n",
    "# The latter is evaluated with MSE\n",
    "session_pred = Dense(n_items, activation='softmax')(drop2) \n",
    "dwell_pred = Dense(1, kernel_initializer='normal')(drop2)\n",
    "predictions = [session_pred, dwell_pred]\n",
    "\n",
    "model = Model(input=inputs, output=predictions)\n",
    "\n",
    "# lr original es 0.0001\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss=custom_loss_wrap(train_data['DwellReps'].max()), optimizer=opt)\n",
    "model.summary()\n",
    "\n",
    "filepath='./DwellTimeModel_checkpoint.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(model):\n",
    "    return [K.get_value(s) for s,_ in model.state_updates]\n",
    "\n",
    "def set_states(model, states):\n",
    "    for (d,_), s in zip(model.state_updates, states):\n",
    "        K.set_value(d, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def get_recall(model, train_generator_map, recall_k=20):\n",
    "\n",
    "\n",
    "    test_dataset = SessionDataset(test_data, itemmap=train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size, test=True)\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    suma_baseline = 0\n",
    "\n",
    "    for feat, label, mask in test_generator:\n",
    "\n",
    "        \n",
    "        print(feat)\n",
    "        \n",
    "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "        target_oh = to_categorical(label, num_classes=loader.n_items)\n",
    "\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        if n%100 == 0:\n",
    "            try:\n",
    "                print(\"{}:{}\".format(n, suma/n))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx] \n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
    "            idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "            if idx2[0] in idx1:\n",
    "                suma += 1\n",
    "\n",
    "    print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrr(model, train_generator_map, mrr_k=20):\n",
    "\n",
    "    test_dataset = SessionDataset(test_data, itemmap = train_generator_map)\n",
    "    test_generator = SessionDataLoader(test_dataset, batch_size=batch_size, test=True)\n",
    "\n",
    "    n = 0\n",
    "    suma = 0\n",
    "    suma_baseline = 0\n",
    "\n",
    "    for feat, label, mask in test_generator:\n",
    "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
    "        input_oh = np.expand_dims(input_oh, axis=1)\n",
    "        target_oh = to_categorical(label, num_classes=loader.n_items)\n",
    "\n",
    "        pred = model.predict(input_oh, batch_size=batch_size)\n",
    "\n",
    "        if n%100 == 0:\n",
    "            try:\n",
    "                print(\"{}:{}\".format(n, suma/n))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for row_idx in range(feat.shape[0]):\n",
    "            pred_row = pred[row_idx] \n",
    "            label_row = target_oh[row_idx]\n",
    "\n",
    "            idx1 = pred_row.argsort()[-mrr_k:][::-1]\n",
    "            idx2 = label_row.argsort()[-1:][::-1]\n",
    "\n",
    "            n += 1\n",
    "            if idx2[0] in idx1:\n",
    "                suma += 1/int((np.where(idx1 == idx2[0])[0]+1))        \n",
    "\n",
    "    print(\"MRR@{} epoch {}: {}\".format(mrr_k, epoch, suma/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0. Total loss: 5.23043. dense_3_loss: 5.16965. dense_4_loss: 0.06078:  68%|██████▊   | 8463/12372 [00:17<00:08, 463.78it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8c4afdb7069c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtarget_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mreal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ma_mysql/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ma_mysql/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ma_mysql/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ma_mysql/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = SessionDataset(train_data)\n",
    "\n",
    "model_to_train = model\n",
    "metrics = model_to_train.metrics_names\n",
    "\n",
    "with tqdm(total=train_samples_qty) as pbar:\n",
    "    for epoch in range(5):\n",
    "        loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
    "        for feat, target, mask in loader:\n",
    "            \n",
    "            session_feat, dwell_feat = feat\n",
    "            session_target, dwell_target = target\n",
    "            \n",
    "            input_oh = to_categorical(session_feat, num_classes=loader.n_items) \n",
    "            input_oh = np.expand_dims(input_oh, axis=1)\n",
    "\n",
    "            target_a = to_categorical(session_target, num_classes=loader.n_items)\n",
    "            target_b = dwell_target\n",
    "            target_oh = [target_a, target_b]\n",
    "            \n",
    "            tr_loss = model_to_train.train_on_batch(input_oh, target_oh)\n",
    "\n",
    "            real_mask = np.ones((batch_size, 1))\n",
    "            for elt in mask:\n",
    "                real_mask[elt, :] = 0\n",
    "\n",
    "            hidden_states = get_states(model_to_train)[0]\n",
    "\n",
    "            hidden_states = np.multiply(real_mask, hidden_states)\n",
    "            hidden_states = np.array(hidden_states, dtype=np.float32)\n",
    "            model_to_train.layers[1].reset_states(hidden_states)\n",
    "            \n",
    "            pbar.set_description(\"Epoch {0}. Total {1}: {2:.5f}. {3}: {4:.5f}. {5}: {6:.5f}\".format(epoch, \n",
    "                                                                                                 metrics[0],\n",
    "                                                                                                 tr_loss[0],\n",
    "                                                                                                 metrics[1],\n",
    "                                                                                                 tr_loss[1],\n",
    "                                                                                                 metrics[2],\n",
    "                                                                                                 tr_loss[2]))\n",
    "            pbar.update(loader.done_sessions_counter)\n",
    "\n",
    "        # get metrics for epoch\n",
    "        get_recall(model_to_train, train_dataset.itemmap)\n",
    "        get_mrr(model_to_train, train_dataset.itemmap)\n",
    "\n",
    "        # save model\n",
    "        model_to_train.save('./DwellTimeEpoch{}.h5'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSC15 con batcher paralelo\n",
    "\n",
    "## Resultados baseline nuestro\n",
    "\n",
    "Cross-entropy, 100 GRU cells\n",
    "\n",
    "Recall@20 epoch 1: 0.6341458834134616\n",
    "\n",
    "Recall@20 epoch 4: 0.69666\n",
    "\n",
    "MRR@20    epoch 1: 0.26131587247895166\n",
    "\n",
    "Recall@20 epoch 2: 0.6583345853365384\n",
    "\n",
    "MRR@20 epoch 2: 0.277304691948809\n",
    "\n",
    "## Resultados con Dwell Time\n",
    "\n",
    "6 horas aprox. por epoca.\n",
    "\n",
    "Cross-entropy, 100 GRU cells \n",
    "\n",
    "Recall@20 epoch 1: 0.6466909555288461\n",
    "\n",
    "MRR@20 epoch 1: 0.27475645038396757\n",
    "\n",
    "## Resultados paper original\n",
    "\n",
    "Cross-entropy, 100 GRU cells \n",
    "\n",
    "Recall@20 epoch 10: 0.6074 \n",
    "\n",
    "MRR@20    epoch 10: 0.2430\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IIC3633_M1_Colab_V2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
